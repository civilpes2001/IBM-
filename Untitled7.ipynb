{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled7.ipynb",
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9FDEOmgV9qO",
        "colab_type": "text"
      },
      "source": [
        "#The Battle of the Neighborhoods - Week 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBD8xXCaV98-",
        "colab_type": "text"
      },
      "source": [
        "Part 1 Download and Explore New York city geographical coordinates dataset\n",
        "Neighborhood has a total of 5 boroughs and 306 neighborhoods. In order to segement the neighborhoods and explore them, we will essentially need a dataset that contains the 5 boroughs and the neighborhoods that exist in each borough as well as the the latitude and logitude coordinates of each neighborhood.\n",
        "\n",
        "Luckily, this dataset exists for free on the web. Link to the dataset: https://geo.nyu.edu/catalog/nyu_2451_34572\n",
        "\n",
        "First, let's download all the dependencies that we will need."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXdCufIKWEgx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np # library to handle data in a vectorized manner\n",
        "\n",
        "import pandas as pd # library for data analsysis\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "\n",
        "import json # library to handle JSON files\n",
        "\n",
        "!conda install -c conda-forge geopy --yes # uncomment this line if you haven't completed the Foursquare API lab\n",
        "from geopy.geocoders import Nominatim # convert an address into latitude and longitude values\n",
        "\n",
        "import requests # library to handle requests\n",
        "from pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe\n",
        "\n",
        "# Matplotlib and associated plotting modules\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.colors as colors\n",
        "\n",
        "!conda install -c conda-forge folium=0.5.0 --yes # uncomment this line if you haven't completed the Foursquare API lab\n",
        "import folium # map rendering library\n",
        "\n",
        "import csv # implements classes to read and write tabular data in CSV form\n",
        "\n",
        "print('Libraries imported.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qph0LLVSWEpf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -q -O 'newyork_data.json' https://ibm.box.com/shared/static/fbpwbovar7lf8p5sgddm06cgipa2rxpe.json\n",
        "print('Data downloaded!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gw5L0p9fWNTD",
        "colab_type": "text"
      },
      "source": [
        "**Load and explore the data**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_RIkM9mWUiN",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "All the relevant data is in the features key, which is basically a list of the neighborhoods. So, define a new variable that includes this data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcuNpPs1WKEn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('newyork_data.json') as json_data:\n",
        "    newyork_data = json.load(json_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbw5w6M5WQbw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "neighborhoods_data = newyork_data['features']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CRK7xLdWYR6",
        "colab_type": "text"
      },
      "source": [
        "Take a look at the first item in this list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIJUtFHRWR_1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "neighborhoods_data[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIOp3s40Wbsv",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**Tranform the data into a pandas dataframe**<br>\n",
        "The next task is essentially transforming this data of nested Python dictionaries into a pandas dataframe. Start by creating an empty dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhr4bB3zWZ9x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the dataframe columns\n",
        "column_names = ['Borough', 'Neighborhood', 'Latitude', 'Longitude'] \n",
        "\n",
        "# instantiate the dataframe\n",
        "neighborhoods = pd.DataFrame(columns=column_names)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lfzwotg0WgSp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "neighborhoods"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-OfAGouwWkfN",
        "colab_type": "text"
      },
      "source": [
        "Then loop through the data and fill the dataframe one row at a time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5oFYGKTWhvy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for data in neighborhoods_data:\n",
        "    borough = neighborhood_name = data['properties']['borough'] \n",
        "    neighborhood_name = data['properties']['name']\n",
        "        \n",
        "    neighborhood_latlon = data['geometry']['coordinates']\n",
        "    neighborhood_lat = neighborhood_latlon[1]\n",
        "    neighborhood_lon = neighborhood_latlon[0]\n",
        "    \n",
        "    neighborhoods = neighborhoods.append({'Borough': borough,\n",
        "                                          'Neighborhood': neighborhood_name,\n",
        "                                          'Latitude': neighborhood_lat,\n",
        "                                          'Longitude': neighborhood_lon}, ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZTY1NmpWmpk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "neighborhoods.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whgtALBFWqbS",
        "colab_type": "text"
      },
      "source": [
        "Let's make sure that the dataset has all 5 boroughs and 306 neighborhoods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JamGrLeHWohC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "print('The dataframe has {} boroughs and {} neighborhoods.'.format(\n",
        "        len(neighborhoods['Borough'].unique()),\n",
        "        neighborhoods.shape[0]\n",
        "    )\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SW15iDR3Wrzr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "neighborhoods.to_csv('BON1_NYC_GEO.csv',index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdyxm3gEWvTf",
        "colab_type": "text"
      },
      "source": [
        "Use geopy library to get the latitude and longitude values of New York City."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHFejRacWt1s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "address = 'New York City, NY'\n",
        "\n",
        "geolocator = Nominatim(user_agent=\"Jupyter\")\n",
        "location = geolocator.geocode(address)\n",
        "latitude = location.latitude\n",
        "longitude = location.longitude\n",
        "print('The geograpical coordinate of New York City are {}, {}.'.format(latitude, longitude))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KR5bk8GmWyv7",
        "colab_type": "text"
      },
      "source": [
        "The geograpical coordinate of New York City are 40.7308619, -73.9871558.<br>\n",
        "<br>\n",
        "<br>\n",
        "**Create a map of New York with neighborhoods superimposed on top.**<br>\n",
        "<br>\n",
        "\n",
        "**Folium** is a great visualization library. We can zoom into the below map, and click on each circle mark to reveal the name of the neighborhood and its respective borough."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqhrLDhdWw2s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# create map of Toronto using latitude and longitude values\n",
        "map_NewYork = folium.Map(location=[latitude, longitude], zoom_start=10)\n",
        "\n",
        "# add markers to map\n",
        "for lat, lng, borough, neighborhood in zip(neighborhoods['Latitude'], neighborhoods['Longitude'], neighborhoods['Borough'], neighborhoods['Neighborhood']):\n",
        "    label = '{}, {}'.format(neighborhood, borough)\n",
        "    label = folium.Popup(label, parse_html=True)\n",
        "    folium.CircleMarker(\n",
        "        [lat, lng],\n",
        "        radius=5,\n",
        "        popup=label,\n",
        "        color='blue',\n",
        "        fill=True,\n",
        "        fill_color='#3186cc',\n",
        "        fill_opacity=0.7,\n",
        "        parse_html=False).add_to(map_NewYork)  \n",
        "    \n",
        "map_NewYork"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERNeyfyHXE5m",
        "colab_type": "text"
      },
      "source": [
        "### Part 2 Web scrapping of Population and Demographics data of New York city from Wikipedia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7g0NlhuXIUh",
        "colab_type": "text"
      },
      "source": [
        "**A : POPULATION DATA**<br>\n",
        "<br>\n",
        "\n",
        "Web scrapping of Population data from wikipedia page - https://en.wikipedia.org/wiki/New_York_City<br>\n",
        "<br>\n",
        "**Download all the dependencies that is needed.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzsJOYBzW_hn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np # library to handle data in a vectorized manner\n",
        "\n",
        "import pandas as pd # library for data analsysis\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "\n",
        "import json # library to handle JSON files\n",
        "\n",
        "!conda install -c conda-forge geopy --yes # uncomment this line if you haven't completed the Foursquare API lab\n",
        "from geopy.geocoders import Nominatim # convert an address into latitude and longitude values\n",
        "\n",
        "import requests # library to handle requests\n",
        "from pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe\n",
        "\n",
        "# Matplotlib and associated plotting modules\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.colors as colors\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# conda install -c anaconda beautiful-soup --yes\n",
        "from bs4 import BeautifulSoup # package for parsing HTML and XML documents\n",
        "\n",
        "import csv # implements classes to read and write tabular data in CSV form\n",
        "\n",
        "print('Libraries imported.')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytuUg8BtXkGI",
        "colab_type": "text"
      },
      "source": [
        "**Web scrapping of Population data from wikipedia page using BeautifulSoup.**<br>\n",
        "<br>\n",
        "Beautiful Soup is a Python package for parsing HTML and XML documents (including having malformed markup, i.e. non-closed tags, so named after tag soup). It creates a parse tree for parsed pages that can be used to extract data from HTML, which is useful for web scraping."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g66c9CuWXgLr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "website_url = requests.get('https://en.wikipedia.org/wiki/Demographics_of_New_York_City').text\n",
        "soup = BeautifulSoup(website_url,'lxml')\n",
        "table = soup.find('table',{'class':'wikitable sortable'})\n",
        "#print(soup.prettify())\n",
        "\n",
        "headers = [header.text for header in table.find_all('th')]\n",
        "\n",
        "table_rows = table.find_all('tr')        \n",
        "rows = []\n",
        "for row in table_rows:\n",
        "   td = row.find_all('td')\n",
        "   row = [row.text for row in td]\n",
        "   rows.append(row)\n",
        "\n",
        "with open('BON2_POPULATION1.csv', 'w') as f:\n",
        "   writer = csv.writer(f)\n",
        "   writer.writerow(headers)\n",
        "   writer.writerows(row for row in rows if row)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5s422b-RXtqO",
        "colab_type": "text"
      },
      "source": [
        "**Load data from CSV**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArBproIyXrhe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "Pop_data=pd.read_csv('BON2_POPULATION1.csv')\n",
        "Pop_data.drop(Pop_data.columns[[7,8,9,10,11]], axis=1,inplace=True)\n",
        "print('Data downloaded!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CJx9zNxXzNw",
        "colab_type": "text"
      },
      "source": [
        "**Remove whitespaces and rename columns**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpMdh2BoXwDg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Pop_data.columns = Pop_data.columns.str.replace(' ', '')\n",
        "Pop_data.columns = Pop_data.columns.str.replace('\\'','')\n",
        "Pop_data.rename(columns={'Borough':'persons_sq_mi','County':'persons_sq_km'}, inplace=True)\n",
        "Pop_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZD3KIT8KX4OE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Pop_data.rename(columns = {'NewYorkCitysfiveboroughsvte\\n' : 'Borough',\n",
        "                   'Jurisdiction\\n':'County',\n",
        "                   'Population\\n':'Estimate_2017', \n",
        "                   'Landarea\\n':'square_miles',\n",
        "                    'Density\\n':'square_km'}, inplace=True)\n",
        "Pop_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlW2Y2M-X_at",
        "colab_type": "text"
      },
      "source": [
        "**Replace newline('\\n') from each string from left and right sides**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqTY-xBsX7Y4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Pop_data['Borough']=Pop_data['Borough'].replace(to_replace='\\n', value='', regex=True)\n",
        "Pop_data['County']=Pop_data['County'].replace(to_replace='\\n', value='', regex=True)\n",
        "Pop_data['Estimate_2017']=Pop_data['Estimate_2017'].replace(to_replace='\\n', value='', regex=True)\n",
        "Pop_data['square_miles']=Pop_data['square_miles'].replace(to_replace='\\n', value='', regex=True)\n",
        "Pop_data['square_km']=Pop_data['square_km'].replace(to_replace='\\n', value='', regex=True)\n",
        "Pop_data['persons_sq_mi']=Pop_data['persons_sq_mi'].replace(to_replace='\\n', value='', regex=True)\n",
        "Pop_data['persons_sq_km']=Pop_data['persons_sq_km'].replace(to_replace='\\n', value='', regex=True)\n",
        "Pop_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoRd_n8HYDM_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Pop_data.loc[5:,['persons_sq_mi','persons_sq_km']] = Pop_data.loc[2:,['persons_sq_mi','persons_sq_km']].shift(1,axis=1)\n",
        "Pop_data.loc[5:,['square_km','persons_sq_mi']] = Pop_data.loc[2:,['square_km','persons_sq_mi']].shift(1,axis=1)\n",
        "Pop_data.loc[5:,['square_miles','square_km']] = Pop_data.loc[2:,['square_miles','square_km']].shift(1,axis=1)\n",
        "Pop_data.loc[5:,['Estimate_2017','square_miles']] = Pop_data.loc[2:,['Estimate_2017','square_miles']].shift(1,axis=1)\n",
        "Pop_data.loc[5:,['County','Estimate_2017']] = Pop_data.loc[2:,['County','Estimate_2017']].shift(1,axis=1)\n",
        "Pop_data.loc[5:,['Borough','County']] = Pop_data.loc[2:,['Borough','County']].shift(1,axis=1)\n",
        "Pop_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6odVbbPYNAn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Pop_data = Pop_data.fillna('')\n",
        "Pop_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F80WMa8vYZJi",
        "colab_type": "text"
      },
      "source": [
        "**Drop the last row**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wh3vblmDYPtq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i = Pop_data[((Pop_data.County == 'Sources: [2] and see individual borough articles'))].index\n",
        "Pop_data.drop(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFa7tdU3Yd6M",
        "colab_type": "text"
      },
      "source": [
        "**Save dataframe as csv file**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjpWUY8nYURg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Pop_data.to_csv('BON2_POPULATION.csv',index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgtQV82DYjPw",
        "colab_type": "text"
      },
      "source": [
        "**B : DEMOGRAPHICS DATA**<br>\n",
        "<br>\n",
        "\n",
        "We will web scrap Demographics data from wikipedia page - https://en.wikipedia.org/wiki/New_York_City\n",
        "<br>\n",
        "<br>\n",
        "**Web scrapping of Demographics data from wikipedia page using BeautifulSoup.**<br>\n",
        "<br>\n",
        "Beautiful Soup is a Python package for parsing HTML and XML documents (including having malformed markup, i.e. non-closed tags, so named after tag soup). It creates a parse tree for parsed pages that can be used to extract data from HTML, which is useful for web scraping."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6z49b4jNYUbu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "website_url = requests.get('https://en.wikipedia.org/wiki/New_York_City').text\n",
        "soup = BeautifulSoup(website_url,'lxml')\n",
        "table = soup.find('table',{'class':'wikitable sortable collapsible'})\n",
        "#print(soup.prettify())\n",
        "\n",
        "headers = [header.text for header in table.find_all('th')]\n",
        "\n",
        "table_rows = table.find_all('tr')        \n",
        "rows = []\n",
        "for row in table_rows:\n",
        "   td = row.find_all('td')\n",
        "   row = [row.text for row in td]\n",
        "   rows.append(row)\n",
        "\n",
        "with open('NYC_DEMO.csv', 'w') as f:\n",
        "   writer = csv.writer(f)\n",
        "   writer.writerow(headers)\n",
        "   writer.writerows(row for row in rows if row)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRctlpZBY2aj",
        "colab_type": "text"
      },
      "source": [
        "**Load data from CSV**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YcHjtcZYUXt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Demo_data=pd.read_csv('NYC_DEMO.csv')\n",
        "print('Data downloaded!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buIrDpA3Y5Pe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}